{
 "metadata": {
  "name": "",
  "signature": "sha256:fbe64c22f01b2bcaa525de565bcdb1d68716ed72b1e2f76edbc51a27014daec1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introduction\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous module we started with a continuous distribution of a physical property and discretized it into many cells, then when performed a forward simulation that created data from known model parameters. Inversion, of course, is exactly the opposite process. Imagine each model parameter that we had represents a layer in a 1D layered earth. At the surface of the earth we measure the data, and when we invert we do so for the model parameters. Our goal is to take the observed data and recover models that emmulate the real Earth as closely as possible. \n",
      "\n",
      "You may have noticed that the act of discretizing our problem created more cells than data values. In our latter example we produced 20 data points from 1000 model parameters, which is only a few data points and many model parameters. While this was not much of a problem in the forward simulation, when we want to do the inverse process, that is, obtain the model parameters from the data, it is clear that we have many more unknowns than knowns. In short, we have an underdetermined problem, and therefore infinite possible solutions. In mathematical terms, geophysical surveys represent what are called \"ill-posed\" problems. \n",
      "\n",
      "**Defintion: An Ill-posed problem**\n",
      "\n",
      "Although at first blush it is unsatisfying to describe an \"ill-posed\" as a problem that is not a \"well-posed\" problem, but allow me to explain. A *well-posed* problem is a problem in mathematics that must satisfy all three of the following criteria:\n",
      "\n",
      "<ol>\n",
      "<li> A solution exists.\n",
      "<li> The solution is unique.\n",
      "<li> The solution's behaviors change continuously with continuously changing initial conditions.\n",
      "</ol>\n",
      "\n",
      "Any mathematical formuation that does not satisfy all three of the above is, by definition, an ill-posed problem. Since we are dealing with an underdetermined system, I hope that it is clear that we are dealing with an ill-posed problem (i.e., we have no unique solution), and we are going to have to come up with  a method (or methods) that can help us choose from the available solutions. We need to devise an algorithm that can choose the \"best\" model from the infinitely many that are available to us. \n",
      "\n",
      "In short, we are going to have to find an optimum model. More specifically, in the context of most geophysics problems, we are going to use gradient-based optimization. This process involves building a \"model objective function,\" which is a function that casts our inverse problem as an optimization problem. The model objective function consists of two parts, (1) a data misfit (denoted as $\\phi_d$) and (2) a model norm (denoted as $\\phi_m$). These two parts will be elaborated in detail below.\n",
      "\n",
      "Once we have formulated the model obejective function, we will take derivatives and obtain a recovered model. Given that there are certain aspects of mathematics and statistics that may be unfamiliar to some readers, this module will go over the necessary mathematical background, then flesh out the details of the model objective function, and last take first and second derivatives of our model objective function to derive an expression that gives us a solution for our model parameters.\n",
      "\n",
      "This module will be divided into the following sections:\n",
      "\n",
      "<ol>\n",
      "<li> A review of norms.\n",
      "<li> A bit of matrix calculus.\n",
      "<li> A bit of statistics.\n",
      "<li> Describe the parts of the model objective function ($\\phi_d$ and $\\phi_m$).\n",
      "<li> Differentiate the model objective function.\n",
      "<ol/><br>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**A Short note on notation**\n",
      "\n",
      "Just to be clear about notation, in the following unit I will denote matrices as uppercase Latin letters (e.g. $A$), vectors as lowercase Latin letters (e.g. $x$), scalar functions as Greek letters with an argument labeled explicitly (e.g. $\\phi(x)$) and scalars simple as Greek letters (e.g. $\\alpha$). Matrix dimensions and the maximum numbers for indices will also be written with upperase letters (e.g. $N$), and I believe context will allow this to be distinguished from matrices.\n",
      "\n",
      "\n",
      "<-- Sources: http://www.eos.ubc.ca/ubcgif/iag/index.htm SimPEG paper  -->\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "A Review of Norms"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In simple terms, a *norm* is a function that assigns a number (a size or length) to a non-zero vector. You can think of a norm like a ruler that assigns a length to the vector that we are measuring. And just as value we obtain from a ruler depends on our choice of ruler (we can measuring in feet, meters, inches, etc. ) the value that we obtain from measuring a vector will depend on the norm that we use.\n",
      "\n",
      "A commonly used norm that you are likely familiar with is the Euclidean norm that measures the length of a vector $x$ from Pythagoras's theorem: \n",
      "\n",
      "$$\n",
      "\\|x\\|_2 = \\left( \\sum_{i=1}^n x_i^2 \\right)^{1/2}\n",
      "$$\n",
      "\n",
      "Because this is a standard way to measure the magnitude of a vector and has a clear geometric representation, the Euclidean norm is by far the most common. But it is not the only one. In our geophysical context, the norm is a vehicle by which prior information is included. In general, we will need to define a norm that is associated with structure (and by *structure* we mean how a physical property varies in space, or more particularly, how deviations from the background earth can be measured). The norm should, for example, account for things like blocky or smoothly varying properties in the subsurface. As we develop our model objective function, we will be using the Euclidean norm, but it is illustrative to take a moment and see how different norms have particular uses, depending on the information you desire to obtain. \n",
      "\n",
      "**Norm Types and Examples**\n",
      "\n",
      "For a vector $x$ of length $N$, some common norms are given below:\n",
      "<ol>\n",
      "<li> The $L_1$, \"Taxicab\", or \"1-norm\" is simply the sum of the absolute values of all the elements in a vector: $\\|x\\|_1= \\sum_{i=1}^N |x_i|$\n",
      "<li> The $L_2$, \"Euclidean\", or just \"2-norm\", as above, is the sum of the squares of the elements in a vector: $\\|x\\|_2 = \\left( \\sum_{i=1}^N x_i^2 \\right)^{1/2}$. In terms of vectors, the Euclidean norm can be written as the square root of the inner product of two vectors: $\\|x\\|_2 = \\sqrt{x \\cdot x} = \\sqrt{x^T x}$.\n",
      "<li> The P-Norm. For any value of $p$, the P-norm is given by $\\|x\\|_p = \\left( \\sum_{i=1}^N x_i^p \\right)^{1/p}$. Both the $L_1$ and $L_2$ norms given above are simply special cases of this norm. \n",
      "<li> The $\\infty$-norm. The $\\infty$-norm is equal to the absolute value of the maximum entry in our vector: $\\|x\\|_{\\infty} = max[|x_1|, |x_2|,...,|x_N|]$\n",
      "</ol>\n",
      "\n",
      "Just to be clear, note again that the output of our norm is a scalar function that depends on the vector $x$. We can then denote this as $\\phi(x)$. So when we write, say, the square of the 2-norm, we can do so as:\n",
      "\n",
      "$$\n",
      "\\phi(x)^2 = x^T x\n",
      "$$\n",
      "\n",
      "A norm that will prove useful in our geophysical context is a \"finite-difference norm\" that measures differences between adjacent values. When we are dealing with model parameters, this will effectively measure the smoothness of our model. This can be written as follows:\n",
      "\n",
      "$$\n",
      "\\phi(x) = \\sum_{i=1}^{N-1} (x_{i+1} - x_i)\n",
      "$$\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "\n",
      "The Model Norm, $\\phi_m$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "In the previous section, we wrote our norms in terms of some arbitrary vector $x$. Since we are going to build up the model norm, which lives on a one dimensional domain, it is useful to adapt the notation for our purposes. Let's substitute our vector $x$ for a vector $m$ that lives on a 1-D domain that we will call $x$ with cell sizes $dx$, and then write our scalar function using a subscript $\\phi_m$. \n",
      "\n",
      "**Norms of functions**\n",
      "\n",
      "Vectors in a vector space have corresponding analogues with functions in a function space. When we shrink our cells to infinitessimal size, we can still have norms to measure the size of the elements. We can then switch between discreet and continous representations of the Euclidean and finite difference norms that we defined above in the following manner:\n",
      "\n",
      "$$\n",
      "(\\phi_m)^2 = \\sum_{i=1}^N (m_i)^2 \\leftrightarrow \\int (m)^2 dx\n",
      "$$\n",
      "\n",
      "\n",
      "$$\n",
      "\\phi_m = \\sum_{i=1}^{N-1} (m_{i+1} - m_i) \\leftrightarrow  \\int \\left( \\frac{dm}{dx} \\right)  dx\n",
      "$$\n",
      "\n",
      "\n",
      "If we want to rewrite this in terms of a refence model that we will call $m_0$, then we can compare our model $m$ with our reference model by taking the difference between the two. In that case, the norms above are written\n",
      "\n",
      "\n",
      "$$\n",
      "(\\phi_m)^2 = \\sum_{i=1}^N (m_i-m_{0_i})^2 \\leftrightarrow \\int (m-m_0)^2 dx\n",
      "$$\n",
      "\n",
      "\n",
      "$$\n",
      "\\phi_m = \\sum_{i=1}^{N-1} (m_{i+1} - m_i) - (m_{0_{i+1}} - m_{0_{i}}) \\leftrightarrow  \\int \\left( \\frac{d}{dx}(m-m_0) \\right)  dx\n",
      "$$\n",
      "\n",
      "Each of the above norms has a particular meaning. The 2-norm measures the size of the model and the finite difference norm measures its *smoothness* or *flatness*. Since all of this information is useful to use, we will combine each of these norms and add a scaling factor for each, using $\\alpha_s$ and $\\alpha_x$, respectively. This will provide use with the model norm that we will be using for our inversion:\n",
      "\n",
      "$$\n",
      "\\phi_m = \\alpha_s  \\int (m-m_0)^2 dx + \\alpha_x \\int \\left( \\frac{d}{dx}(m-m_0) \\right)  dx\n",
      "$$\n",
      "\n",
      "In discretized notation this can get messy, so let us write our vector $m-m_0$ more compactly and simply define a vector of residuals $r$ as $r=m-m_0$. Now our model norm is \n",
      "\n",
      "$$\n",
      "\\phi_m = \\| \\alpha_s \\sum_{i=1}^N (r_i)^2 + \\alpha_x \\sum_{i=1}^{N-1} (r_{i+1} - r_i) \\|_2^2\n",
      "$$\n",
      "\n",
      "The summation notation can be a bit messy, so it is easier to write this out as a matrix vector product, where we can define a matrix $W_m$ as a partitioned matrix of two matrices, $\\alpha_s I$ and $\\alpha_x W_x$ as follows:\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "W_m=\n",
      "\\begin{bmatrix}\n",
      "    \\alpha_s I \\\\\n",
      "    \\alpha_x W_x\n",
      "\\end{bmatrix}\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "Now our model norm can be written as the 2-norm of a vector $\\frac{1}{2} W_m r$ or  $\\frac{1}{2} W_m (m-m_0)$ as follows:\n",
      "\n",
      "$$\n",
      "\\phi_m = \\frac{\\beta}{2}\\|W_m(m-m_0)\\|_2^2\n",
      "$$\n",
      "\n",
      "**A trival Example**\n",
      "\n",
      "Let's consider a very simple case where we only have two model parameters in our vector $m$:\n",
      "\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "m=\n",
      "\\begin{bmatrix}\n",
      "    m_1 \\\\\n",
      "    m_2\n",
      "\\end{bmatrix}\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "And we have some reference model that we will call $m_0$\n",
      "\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "m_0=\n",
      "\\begin{bmatrix}\n",
      "    m_{0_1} \\\\\n",
      "    m_{0_2}\n",
      "\\end{bmatrix}\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "The residual between the two is a vector $r$\n",
      "\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "r= m - m_0 =\n",
      "\\begin{bmatrix}\n",
      "    r_1 \\\\\n",
      "    r_2\n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "    m_1 - m_{0_1} \\\\\n",
      "    m_2 - m_{0_2}\n",
      "\\end{bmatrix}\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "The term that mesures the size of our model is given by:\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "\\alpha_s I=\n",
      "\\begin{bmatrix}\n",
      "    \\alpha_s \\quad 0\\\\\n",
      "    0 \\quad \\alpha_s\n",
      "\\end{bmatrix}\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "Since our vector is small, the term that measures the smoothness will be:\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "\\alpha_x W_x=\n",
      "\\alpha_x\n",
      "\\begin{bmatrix}\n",
      "    -1 \\quad 1\n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "    -\\alpha_x \\quad \\alpha_x\n",
      "\\end{bmatrix}\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "Gathering both in our partitioned matix $W_m$ yields:\n",
      "\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "W_m=\n",
      "\\begin{bmatrix}\n",
      "    \\alpha_s \\quad 0\\\\\n",
      "    0 \\quad \\alpha_s\\\\\n",
      "    -\\alpha_x \\quad \\alpha_x \n",
      "\\end{bmatrix}\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "Now we apply this to the residuals:\n",
      "\n",
      "\\begin{equation}\n",
      "W_m r =\n",
      "\\begin{split}\n",
      "\\begin{bmatrix}\n",
      "    \\alpha_s \\quad 0\\\\\n",
      "    0 \\quad \\alpha_s\\\\\n",
      "    -\\alpha_x \\quad \\alpha_x \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    r_1 \\\\\n",
      "    r_2\n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "    \\alpha_s r_1 \\\\\n",
      "    \\alpha_s r_2 \\\\\n",
      "    \\alpha_x (r_2-r_1)\n",
      "\\end{bmatrix}\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "Last, we obtain the model norm by taking the 2-norm of this vector and scale it by $\\frac{\\beta}{2}$:\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "\\phi_m = \\frac{\\beta}{2}\n",
      "\\begin{bmatrix}\n",
      "    \\alpha_s r_1 \\quad\n",
      "    \\alpha_s r_2 \\quad\n",
      "    \\alpha_x (r_2-r_1)\n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    \\alpha_s r_1 \\\\[0.6em]\n",
      "    \\alpha_s r_2 \\\\[0.6em]\n",
      "    \\alpha_x (r_2-r_1)\n",
      "\\end{bmatrix}\n",
      "= \\frac{\\beta}{2} \\left( (\\alpha_s r_1)^2 + (\\alpha_s r_2)^2 + (\\alpha_x (r_2-r_1))^2 \\right)\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "As a final step, we can substitute back $m-m_0$ for $r$\n",
      "\n",
      "$$\n",
      "\\phi_m = \\frac{\\beta}{2} \\left( (\\alpha_s (m_1 - m_{0_1}))^2 + (\\alpha_s (m_2 - m_{0_2}))^2 + (\\alpha_x ((m_2 - m_{0_2})-(m_1 - m_{0_1})))^2 \\right)\n",
      "$$\n",
      "\n",
      "\n",
      "   "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The Data Misfit, $\\phi_d$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A *misfit* describes how close synthetic data matches measurements that are made in the field. Traditionally this term refers to the difference between the measured data and the predicted data. If these two quantities are sufficiently close, then we consider the model to be a viable candidate for the solution to our problem.\n",
      "\n",
      "However, because the data are inaccurate, we know that any model that reproduces those data exactly is not feasible. A realistic goal, rather, is to find a model whose predicted data are consistent with the errors in the observations, and this requires incorporating knowledge about the noise and uncertainties in the field data. The concept of fitting the data means that some estimate of the \u201cnoise\u201d be available. Unfortunately \u201cnoise\u201d within the context of inversion is everything that cannot be accounted for by a compatible relationship between the model and the data. \n",
      "\n",
      "A standard approach is to assume that each datum, $d_i$, contains errors that can be described as Gaussian with a standard deviation $\\epsilon_i$. It is important to give a significant amount of thought towards assigning standard deviations in the data, but a reasonable starting point is to assign each $\\epsilon_i$ as $\\epsilon_i= floor +\\%|d_i|$. \n",
      "\n",
      "Incorporating both the differences between predicted and measured data and a measure of the uncertainties in the data yields our misfit function, $\\phi_d$ and it is written as follows:\n",
      "\n",
      "$$\n",
      "\\phi_d (m) = \\frac{1}{2} \\sum_{i=1}^N \\left( \\frac{F[m] -d_i^{obs} }{\\epsilon_i}\\right)^2 = \\frac{1}{2} \\|W_d(F[m] - d^{obs}) \\|_2^2\n",
      "$$ \n",
      "\n",
      "Note that the right hand size of the equation is written as a matrix-vector product (whose output is, of course, a vector), with each $\\epsilon_i$ in the denominator placed on as elements on a diagonal matrix $W_d$, that is $W_d = diag(\\epsilon)$. The notation here can be a bit confusing at first, but what the right hand side is saying is \"take half of the 2-norm of a vector that is the product of a diagonal matrix $W_d$ and a vector obtained by the difference between $F[m] - d^{obs}$.\"\n",
      "\n",
      "Next, if we return to linear problem from the previous section where our forward operator was simply a matrix of kernel functions, we can substitute $F[m]$ with $G$ and obtain\n",
      "$$\n",
      "\\phi_d (m) = \\frac{1}{2} \\sum_{i=1}^N \\left( \\frac{Gm -d_i^{obs} }{\\epsilon_i}\\right)^2 = \\frac{1}{2} \\|W_d(Gm - d^{obs}) \\|_2^2\n",
      "$$ \n",
      "\n",
      "Now that we have  defined a measure of misfit, the next task is to determine a tolerance value, such that if the misfit is about equal to that value, then we have an acceptable fit. Suppose that the standard deviations are known and that errors are Gaussian, then $\\phi_d$ becomes a $\\chi_N^2$ variable with $N$ degreess of freedom. This is a well-known quantity with an expected value $E[\\chi_N^2]=N$ and a standard deviation of $\\sqrt{2N}$.\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "A Bit of Matrix Calculus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we differentiate the model objective function, let's take a moment to explore some properties of matrix calculus. I will list them first here and expound below:\n",
      "<ol>\n",
      "<li> Let $y=Ax$, with $A$ independent of $x$. Then  $\\frac{\\partial y}{\\partial x}=A$.\n",
      "<li> Let $y=Ax$, with $x$ as a function of $z$, and $A$ independent of $x$ and $z$. Then $\\frac{\\partial y}{\\partial z}=A \\frac{\\partial x}{\\partial z}$\n",
      "<li> Let a scalar $\\alpha=y^T A x$. Then  $\\frac{\\partial \\alpha}{\\partial x} = y^T A$, and $\\frac{\\partial \\alpha}{\\partial y} =x^T A$.\n",
      "<li> For the quadratic form  $\\alpha=x^T A x$, then  $\\frac{\\partial \\alpha}{\\partial x} = x^T(A + A^T)$.\n",
      "<li> For the quadratic form with $A$ symmetric, then  $\\frac{\\partial \\alpha}{\\partial x} = 2x^T A$.\n",
      "<li> For the scalar $\\alpha=y^T x$ and both $y$ and $x$ are functions of $z$ then $\\frac{\\partial \\alpha}{\\partial z} = \\frac{\\partial \\alpha}{\\partial y} \\frac{\\partial y}{\\partial z} + \\frac{\\partial \\alpha}{\\partial x}\\frac{\\partial x}{\\partial z} = x^T \\frac{\\partial y}{\\partial z} + y^T \\frac{\\partial x}{\\partial z}$.\n",
      "<li> For the scalar $\\alpha=x^T x$ with $x$ as a function of $z$, then  $\\frac{\\partial \\alpha}{\\partial z} = 2x^T \\frac{\\partial x}{\\partial z}$\n",
      "<li> For the scalar $\\alpha=y^T A x$ with $y$ and $x$ as functions of $z$, then \n",
      "$\\frac{\\partial \\alpha}{\\partial z} = x^T A^T \\frac{\\partial y}{\\partial z} + y^T A  \\frac{\\partial x}{\\partial z}$\n",
      "<li> For the scalar $\\alpha=x^T A x$, with $x$ a function of $z$ then $\\frac{\\partial \\alpha}{\\partial z}=2x^T A \\frac{\\partial x}{\\partial z}$ \n",
      "<li> If $A$ is an invertible square matrix with elements that are functions of the scalar parameter $\\alpha$ m then $\\frac{\\partial A^{-1}}{\\partial \\alpha} = -A^{-1}\\frac{\\partial A}{\\partial \\alpha}A^{-1}$\n",
      "</ol>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $\\mathbf{y}=\\psi(\\mathbf{x})$ be a vector valued function where $\\mathbf{y}$ is an $m$ element vector and $\\mathbf{x}$ is an $n$ element vector. The derivative of $\\mathbf{y}$ with respect to $\\mathbf{x}$ is given by:\n",
      "\n",
      "\\begin{equation}\n",
      "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} =\n",
      "\\begin{bmatrix}\n",
      "    \\frac{\\partial y_1}{\\partial x_1}  & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n}  \\\\\n",
      "    \\frac{\\partial y_2}{\\partial x_1}  & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n}  \\\\\n",
      "    \\vdots                             & \\vdots                            & \\ddots & \\vdots                             \\\\ \n",
      "    \\frac{\\partial y_m}{\\partial x_1}  & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}  \\\\  \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "To put it a litle more compactly, what we have is a matrix with a typical element $a_{ij}$ as:\n",
      "\n",
      "\\begin{equation}\n",
      "a_{ij} = \\frac{\\partial y_i}{\\partial x_j}\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "This, you may recognize, is the Jacobian, the matrix of all first-order partial derivatives.\n",
      "\n",
      "**Example 1**\n",
      "\n",
      "To get a feeling for the mathematics, let's look at an example. Let $\\mathbf{y}$ be the following:\n",
      "\n",
      "\\begin{equation}\n",
      "\\mathbf{y}= \n",
      "\\begin{bmatrix}\n",
      "    2x_1 + x_2 \\\\\n",
      "    x_1 - x_2  \\\\\n",
      "    2x_1 - 3x_2\\\\ \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "Then \n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "a_{11} & = \\frac{\\partial y_1}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} \\left(2x_1 + x_2\\right) = 2 \\\\[0.4em]\n",
      "a_{12} & = \\frac{\\partial y_1}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} \\left(2x_1 + x_2\\right) = 1 \\\\[0.4em]\n",
      "a_{21} & = \\frac{\\partial y_2}{\\partial x_1} = \\frac{\\partial}{\\partial x_2} \\left(x_1 - x_2\\right) = 1 \\\\[0.4em]\n",
      "a_{22} & = \\frac{\\partial y_2}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} \\left(x_1 - x_2\\right) = -1 \\\\[0.4em]\n",
      "a_{31} & = \\frac{\\partial y_3}{\\partial x_1} = \\frac{\\partial}{\\partial x_2} \\left(2x_1 - 3x_2\\right) = 2 \\\\[0.4em]\n",
      "a_{32} & = \\frac{\\partial y_3}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} \\left(2x_1 - 3x_2\\right) = -3 \\\\[0.4em]\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "So our Jacobian Matrix $A$ is\n",
      "\n",
      "\\begin{equation}\n",
      "A =\n",
      "\\begin{bmatrix}\n",
      "    2 & 1 \\\\\n",
      "    1 & -1 \\\\\n",
      "    1 & -3 \\\\    \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "**Derivatives of Matrix-Vector Products**\n",
      "\n",
      "Now let's perform some derivatives on matrix-vector operations. Let $y=Ax$ where $y$ is $m \\times 1$ and $x$ is $n \\times 1$ and $A$ is not dependent on $x$. Then\n",
      "\n",
      "\\begin{equation}\n",
      "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = A\n",
      "\\end{equation}\n",
      "\n",
      "**Example 2**\n",
      "\n",
      "Let's take a two-by-two example, $y=Ax$, with a matrix $A$:\n",
      "\n",
      "\\begin{equation}\n",
      "A =\n",
      "\\begin{bmatrix}\n",
      "    1 & 2 \\\\\n",
      "    3 & 4 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "And a vector $x$:\n",
      "\n",
      "\\begin{equation}\n",
      "x =\n",
      "\\begin{bmatrix}\n",
      "    x_1 \\\\\n",
      "    x_2 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "So our matrix equation gives\n",
      "\n",
      "\\begin{equation}\n",
      "y=Ax=\n",
      "\\begin{bmatrix}\n",
      "    x_1 + 2x_2 \\\\\n",
      "    3x_1 + 4x_2\\\\ \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "As in the previous example, we take the derivatives as follows:\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "a_{11} & = \\frac{\\partial y_1}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} \\left(x_1 + 2x_2\\right) = 1 \\\\[0.4em]\n",
      "a_{12} & = \\frac{\\partial y_1}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} \\left(x_1 + 2x_2\\right) = 2 \\\\[0.4em]\n",
      "a_{21} & = \\frac{\\partial y_2}{\\partial x_1} = \\frac{\\partial}{\\partial x_2} \\left(3x_1 + 4x_2\\right) = 3 \\\\[0.4em]\n",
      "a_{22} & = \\frac{\\partial y_2}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} \\left(3x_1 + 4x_2\\right) = 4 \\\\[0.4em]\n",
      " \\\\[0.4em]\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "Note that our result gives us $A$ back again:\n",
      "\n",
      "\\begin{equation}\n",
      "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = A =\n",
      "\\begin{bmatrix}\n",
      "    1 & 2 \\\\\n",
      "    3 & 4 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "**The Chain-Rule and Matrix-Vector Products**\n",
      "\n",
      "The chain rule for matrix-vector products is... \n",
      "\n",
      "This time, let $y=Ax$, where $y$ is $m \\times 1$, $x$ is $n \\times 1$ and is a function of $z$, and $A$ is a matrix of dimensions $m \\times n$ and is independent of bith $x$ and $z$, then the derivative of $y$ with respect to $z$ is as follows:\n",
      "\n",
      "\\begin{equation}\n",
      " \\frac{\\partial y}{\\partial z} =\\frac{\\partial y}{\\partial x}  \\frac{\\partial x}{\\partial z} = A \\frac{\\partial x}{\\partial z}\n",
      "\\end{equation\n",
      "\n",
      "Again, let's consider an example of the form $y=Ax$, and this time we will take a vector $z$:\n",
      "\n",
      "\\begin{equation}\n",
      "z=\n",
      "\\begin{bmatrix}\n",
      "    z_1 \\\\ z_2\n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "the matrix $A$ from before:\n",
      "\n",
      "\\begin{equation}\n",
      "A =\n",
      "\\begin{bmatrix}\n",
      "    1 & 2 \\\\\n",
      "    3 & 4 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "and $x$ is a function of $z$ in the following form:\n",
      "\n",
      "\\begin{equation}\n",
      "x=\n",
      "\\begin{bmatrix}\n",
      "    3z_1 + 2z_2 \\\\\n",
      "    z_1 - 2z_2\\\\ \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "Using the definition for the Jacobian, we have\n",
      "\n",
      "\\begin{equation}\n",
      "\\frac{\\partial x}{\\partial z} =\n",
      "\\begin{bmatrix}\n",
      "    \\frac{\\partial x_1}{\\partial z_1}  & \\frac{\\partial x_1}{\\partial z_2}  \\\\\n",
      "    \\frac{\\partial y_2}{\\partial x_1}  & \\frac{\\partial y_2}{\\partial x_2}  \\\\\n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "and \n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "\\frac{\\partial x_1}{\\partial z_1} & = \\frac{\\partial}{\\partial z_1} \\left(3z_1 + 2z_2\\right) = 3 \\\\[0.4em]\n",
      "\\frac{\\partial x_1}{\\partial z_2} & = \\frac{\\partial}{\\partial z_2} \\left(3z_1 + 2z_2\\right) = 2 \\\\[0.4em]\n",
      "\\frac{\\partial x_2}{\\partial z_1} & = \\frac{\\partial}{\\partial z_1} \\left(z_1 - 2z_2\\right) = 1  \\\\[0.4em]\n",
      "\\frac{\\partial x_2}{\\partial z_2} & = \\frac{\\partial}{\\partial z_2} \\left(z_1 - 2z_2\\right) = -2 \\\\[0.4em]\n",
      " \\\\[0.4em]\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "So our matrix is:\n",
      "\n",
      "\\begin{equation}\n",
      "\\frac{\\partial x}{\\partial z} =\n",
      "\\begin{bmatrix}\n",
      "    3 & 2 \\\\   \n",
      "    1 & -2 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "The product of $A \\frac{\\partial x}{\\partial z} $ is \n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{bmatrix}\n",
      "    1 & 2 \\\\   \n",
      "    3 & 4 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    3 & 2 \\\\   \n",
      "    1 & -2 \\\\ \n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "    5 & -2 \\\\   \n",
      "    13 & -2 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "Now consider the equation $y=Ax$: \n",
      "\n",
      "\\begin{equation}\n",
      "y=Ax=\n",
      "\\begin{bmatrix}\n",
      "    1 & 2 \\\\   \n",
      "    3 & 4 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    x_1 \\\\   \n",
      "    x_2 \\\\ \n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "    1 & 2 \\\\   \n",
      "    3 & 4 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    3z_1 + 2z_2 \\\\   \n",
      "    z_1 - 2z_2 \\\\ \n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "    5z_1 - 2z_2 \\\\   \n",
      "    13z_1 - 2z_2 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "Now taking the derivative with respect to $z$ gives:\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "\\frac{\\partial y_1}{\\partial z_1} & = \\frac{\\partial}{\\partial z_1} \\left(5z_1 - 2z_2\\right) = 5 \\\\[0.4em]\n",
      "\\frac{\\partial y_1}{\\partial z_2} & = \\frac{\\partial}{\\partial z_2} \\left(5z_1 - 2z_2\\right) = 13 \\\\[0.4em]\n",
      "\\frac{\\partial y_2}{\\partial z_1} & = \\frac{\\partial}{\\partial z_1} \\left(13z_1 - 2z_2\\right)= -2 \\\\[0.4em]\n",
      "\\frac{\\partial y_2}{\\partial z_2} & = \\frac{\\partial}{\\partial z_2} \\left(13z_1 - 2z_2\\right) = -2 \\\\[0.4em]\n",
      " \\\\[0.4em]\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "And this give is, as before:\n",
      "\n",
      "\\begin{equation}\n",
      "\\frac{\\partial y}{\\partial z} =\n",
      "\\begin{bmatrix}\n",
      "    5 & -2 \\\\   \n",
      "    13 & -2 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "So $\\frac{\\partial y}{\\partial z}=\\frac{\\partial y}{\\partial x}\\frac{\\partial x}{\\partial z}=A\\frac{\\partial x}{\\partial z}$\n",
      "\n",
      "**Scalar** \n",
      "\n",
      "Let the scalar $\\alpha$ be represented by\n",
      "\n",
      "\\begin{equation}\n",
      "\\alpha = y^T A x\n",
      "\\end{equation}\n",
      "\n",
      "Then the derivative\n",
      "\n",
      "\\begin{equation}\n",
      "\\frac{\\partial \\alpha}{\\partial x} = y^T A\n",
      "\\end{equation}\n",
      "\n",
      "and \n",
      "\n",
      "\\begin{equation}\n",
      "\\frac{\\partial \\alpha}{\\partial y} = x^T A\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "Examples:\n",
      "\n",
      "Let's take our example matrix and vectors be:\n",
      "\n",
      "\\begin{equation}\n",
      "A=\n",
      "\\begin{bmatrix}\n",
      "    1 & 2\\\\   \n",
      "    2 & 1\\\\ \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "\\begin{equation}\n",
      "x=\n",
      "\\begin{bmatrix}\n",
      "    x_1 \\\\   \n",
      "    x_2 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "\\begin{equation}\n",
      "y=\n",
      "\\begin{bmatrix}\n",
      "    y_1 \\\\   \n",
      "    y_2 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "So $\\alpha$ is given by \n",
      "\n",
      "\\begin{equation}\n",
      "\\alpha=y^TAx\n",
      "\\begin{bmatrix}\n",
      "    y_1  y_2 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    1 & 2 \\\\\n",
      "    2 & 1 \\\\     \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    x_1 & x_2 \\\\ \n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "    y_1  y_2 \\\\ \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    x_1 + 2x_2 \\\\ \n",
      "    2x_1 + x_2 \\\\     \n",
      "\\end{bmatrix}\n",
      "=\n",
      "y_1 x_1 + 2 y_1 x_2 + 2 x_1 y_2 + y_2 x_2\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "Taking the derivative with respect to $y$ gives:\n",
      "\n",
      "\\begin{equation}\n",
      "\\frac{\\partial \\alpha}{\\partial y} =\n",
      "\\begin{bmatrix}\n",
      "    x_1 + 2x_2 \\\\ \n",
      "    2x_1 + x_2 \\\\     \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "Compare this with computing $x^T A$\n",
      "\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{bmatrix}\n",
      "    x_1 x_2     \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    1 & 2 \\\\ \n",
      "    2 & 1 \\\\ \n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "    x_1 + 2x_2 \\\\ \n",
      "    2x_1 + x_2 \\\\     \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "Taking the derivative with respect to $x$ gives:\n",
      "\n",
      "\\begin{equation}\n",
      "\\frac{\\partial \\alpha}{\\partial x} =\n",
      "\\begin{bmatrix}\n",
      "    x_1 + 2x_2 \\\\ \n",
      "    2x_1 + x_2 \\\\     \n",
      "\\end{bmatrix}\n",
      "\\end{equation}\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Model Objective Function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we go back and recall what was discussed in the introduction, the model objective function casts the inverse problem as an optimization problem, and as mentioned, we will be using gradient-based optimization, so we will need to take derivatives. The complete model objective function that we are dealing with will contain both the data misfit and the model norm. This means that we can write it as $\\phi$ as the sum of the two and then differenetiate:\n",
      "\n",
      "$$\n",
      "\\phi = \\phi_d + \\beta \\phi_m\n",
      "$$\n",
      "For the linear problem we are considering\n",
      "$$\n",
      "\\phi_d = \\frac{1}{2}\\| W_d (Gm-d^{obs})\\|_2^2 = \\frac{1}{2}(Gm-d^{obs})^T W_d^T W_d (Gm-d^{obs})\n",
      "$$\n",
      "and\n",
      "$$\n",
      "\\phi_m = \\frac{1}{2} \\|W_m (m-m_{ref}) \\|^2_2 = \\frac{1}{2}(m-m_{ref})^T W_m^T W_m (m-m_{ref})\n",
      "$$\n",
      "\n",
      "To simplify the terms and see the math a little more clearly, let's note that $W_d(Gm-d^{obs})$, and $\\beta W_m(m-m_{ref})$ are simply vectors. And since we are taking the square of the 2-norm, all that we are really doing is taking the dot product of each vector with itself. So let $z=W_d(Gm-d^{obs})$, and let $y=W_m(m-m_{ref})$ where both $z$ and $y$ vectors are functions of $m$. So then:\n",
      "\n",
      "$$\n",
      "\\phi_d =  \\frac{1}{2}\\|z\\|_2^2 = \\frac{1}{2}z^T z \n",
      "$$<br>\n",
      "$$\n",
      "\\phi_m =  \\frac{1}{2}\\|y\\|_2^2 =\\frac{1}{2}y^T y \n",
      "$$\n",
      "\n",
      "\n",
      "To minimize this, we want to look at $\\nabla \\phi$ $\\nabla \\nabla \\phi$. Using our compact expressions:\n",
      "$$\n",
      "\\phi = \\phi_d + \\beta \\phi_m = \\frac{1}{2}z^Tz + \\beta \\frac{1}{2}y^Ty \\\\ \n",
      "$$\n",
      "\n",
      "Taking the derivative with respect to $m$ yields:\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "\\frac{d \\phi}{dm}&  = \\frac{1}{2}  \\left(z^T \\frac{dz}{dm} +  z^T \\frac{dz}{dm} + \\beta y^T \\frac{dy}{dm} + \\beta y^T \\frac{dy}{dm}\\right)\\\\\\\\[0.6em]\n",
      "& = z^T \\frac{dz}{dm} + \\beta y^T \\frac{dy}{dm}\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "Note that \n",
      "$$\\frac{dz}{dm} = \\frac{d}{dm}(W_d(Gm-d^{obs})) = W_d G $$ \n",
      "\n",
      "and \n",
      "\n",
      "$$ \\frac{dy}{dm} = \\frac{d}{dm}(W_m (m-m_{ref})) = W_m $$\n",
      "\n",
      "Next, let's substitute both derivatives, our expressions for $z$ and $y$, apply the transposes, and rearange:<br>\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "\\frac{d \\phi}{dm} & = z^T \\frac{dz}{dm} + \\beta y^T \\frac{dy}{dm} \\\\[0.6em]\n",
      " & = (W_d(Gm-d^{obs}))^T W_d G + \\beta (W_m (m-m_{ref}))^T W_m\\\\[0.6em]\n",
      " & = (Gm-d^{obs})^T W_d^T W_d G + \\beta (m-m_{ref})^T W_m^T W_m \\\\[0.6em]\n",
      " & = ((Gm)^T - d^T) W_d^T W_d G + \\beta (m^T-m_{ref}^T)W_m^T W_m \\\\[0.6em]\n",
      " & = (m^T G^T - d^T) W_d^T W_d G + \\beta m^T W_m^T W_m - \\beta  m_{ref}^T W_m^T W_m \\\\[0.6em]\n",
      " & = m^T G^T W_d^T W_d G  - d^T W_d^T W_d G + \\beta m^T W_m^T W_m - \\beta  m_{ref}^T W_m^T W_m\\\\[0.6em]\n",
      " & = m^T G^T W_d^T W_d G  + \\beta m^T W_m^T W_m - d^T W_d^T W_d G - \\beta  m_{ref}^T W_m^T W_m\n",
      " \\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "Now we have an expression for the derivative of our equation that we can work with. Setting the gradient to zero and gathering like terms gives:<br>\n",
      "\n",
      "\\begin{equation} \n",
      "\\begin{split}\n",
      " m^T G^T W_d^T W_d G  + \\beta m^T W_m^T W_m = d^T W_d^T W_d G + \\beta  m_{ref}^T W_m^T W_m\\\\[0.6em]\n",
      " (G^T W_d^T W_d G  + \\beta W_m^T W_m)m = G^T W_d^T W_d d + \\beta  W_m^T W_m m_{ref}\\\\[0.6em]\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "From here we can do two things. First, we can solve for $m$, our recovered model:\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      " m = (G^T W_d^T W_d G  + \\beta W_m^T W_m)^{-1} (G^T W_d^T W_d d + \\beta  W_m^T W_m m_{ref})\\\\[0.6em]\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "Second, we can get the second derivative simply from the bracketed terms in the left hand side of the equation above:\n",
      "\\begin{equation} \n",
      "\\frac{d^2 \\phi}{dm^2} = G^T W_d^T W_d G  + \\beta W_m^T W_m\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}