{
 "metadata": {
  "name": "",
  "signature": "sha256:2108b703e402898193273d59e0caefd84f10e589cbe190916b51b167bf55a505"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introduction\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous module we started with a continuous distribution of a physical property and discretized it into many cells, then when performed a forward simulation that created data from known model parameters. Inversion, of course, is exactly the opposite process. Imagine each model parameter that we had represents a layer in a 1D layered earth. At the surface of the earth we measure the data, and when we invert we do so for the model parameters. Our goal is to take the observed data and recover models that emmulate the real Earth as closely as possible. \n",
      "\n",
      "You may have noticed that the act of discretizing our problem created more cells than data values. In our latter example we produced 20 data points from 1000 model parameters, which is only a few data points and many model parameters. While this was not much of a problem in the forward simulation, when we want to do the inverse process, that is, obtain the model parameters from the data, it is clear that we have many more unknowns than knowns. In short, we have an underdetermined problem, and therefore infinite possible solutions. In mathematical terms, geophysical surveys represent what are called \"ill-posed\" problems. \n",
      "\n",
      "An \"ill-posed\" as a problem that is not a \"well-posed\" problem. A *well-posed* problem is a problem in mathematics that must satisfy all three of the following criteria:\n",
      "\n",
      "<ol>\n",
      "<li> A solution exists.\n",
      "<li> The solution is unique.\n",
      "<li> The solution's behaviors change continuously with continuously changing initial conditions.\n",
      "</ol>\n",
      "\n",
      "Any mathematical formuation that does not satisfy all three of the above is, by definition, an ill-posed problem. Since we are dealing with an underdetermined system, I hope that it is clear that we are dealing with an ill-posed problem (i.e., we have no unique solution), and we are going to have to come up with  a method (or methods) that can help us choose from the available solutions. We need to devise an algorithm that can choose the \"best\" model from the infinitely many that are available to us. \n",
      "\n",
      "In short, we are going to have to find an optimum model. More specifically, in the context of most geophysics problems, we are going to use gradient-based optimization. This process involves building an *objective function*, which is a function that casts our inverse problem as an optimization problem. We will build an objective function consisting of two parts:\n",
      "\n",
      "$$\n",
      "\\phi = \\phi_d + \\beta \\phi_m\n",
      "$$\n",
      "\n",
      "Where the terms on the right hand side are (1) a data misfit (denoted as $\\phi_d$) and (2) a model regularization (denoted as $\\phi_m$). These two parts will be elaborated in detail below.\n",
      "\n",
      "Once we have formulated the model obejective function, we will take derivatives and obtain a recovered model.this module will flesh out the details of the model objective function, and then take first and second derivatives to derive an expression that gives us a solution for our model parameters.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The Data Misfit, $\\phi_d$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A *misfit* describes how close synthetic data matches measurements that are made in the field. Traditionally this term refers to the difference between the measured data and the predicted data. If these two quantities are sufficiently close, then we consider the model to be a viable candidate for the solution to our problem. Because the data are inaccurate, a model that reproduces those data exactly is not feasible. A realistic goal, rather, is to find a model whose predicted data are consistent with the errors in the observations, and this requires incorporating knowledge about the noise and uncertainties. The concept of fitting the data means that some estimate of the \u201cnoise\u201d be available. Unfortunately \u201cnoise\u201d within the context of inversion is everything that cannot be accounted for by a compatible relationship between the model and the data. More specifically, noise refers to (1) noise from data aquisition in the field, (2) uncertainty in source and receiver locations, (3) numerical error, (4) physical assumptions about our model that do not capture all of the physics. \n",
      "\n",
      "A standard approach is to assume that each datum, $d_i$, contains errors that can be described as Gaussian with a standard deviation $\\epsilon_i$. It is important to give a significant amount of thought towards assigning standard deviations in the data, but a reasonable starting point is to assign each $\\epsilon_i$ as $\\epsilon_i= floor +\\%|d_i|$. \n",
      "\n",
      "Incorporating both the differences between predicted and measured data and a measure of the uncertainties in the data yields our misfit function, $\\phi_d$:\n",
      "\n",
      "$$\n",
      "\\phi_d (m) = \\frac{1}{2} \\sum_{i=1}^N \\left( \\frac{F[m] -d_i^{obs} }{\\epsilon_i}\\right)^2 = \\frac{1}{2} \\|W_d(F[m] - d^{obs}) \\|_2^2\n",
      "$$ \n",
      "\n",
      "Note that the right hand size of the equation is written as a matrix-vector product, with each $\\epsilon_i$ in the denominator placed as elements on a diagonal matrix $W_d$, as follows:\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "W_d = \n",
      "\\begin{bmatrix}\n",
      "    \\frac{1}{\\epsilon_1} & 0 & 0 & \\cdots & 0\\\\\n",
      "    0 & \\frac{1}{\\epsilon_2} & 0 & \\cdots & 0\\\\\n",
      "    0 & 0 & \\frac{1}{\\epsilon_3} & \\cdots & \\vdots\\\\\n",
      "    0 & 0 & 0 & \\ddots & \\frac{1}{\\epsilon_M}\\\\    \n",
      "\\end{bmatrix}\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "If we return to linear problem from the previous section where our forward operator was simply a matrix of kernel functions, we can substitute $F[m]$ with $G$ and obtain\n",
      "$$\n",
      "\\phi_d (m) = \\frac{1}{2} \\sum_{i=1}^N \\left( \\frac{(Gm)_i -d_i^{obs} }{\\epsilon_i}\\right)^2 = \\frac{1}{2} \\|W_d(Gm - d^{obs}) \\|_2^2\n",
      "$$ \n",
      "\n",
      "Now that we have  defined a measure of misfit, the next task is to determine a tolerance value, such that if the misfit is about equal to that value, then we have an acceptable fit. Suppose that the standard deviations are known and that errors are Gaussian, then $\\phi_d$ becomes a $\\chi_N^2$ variable with $N$ degreess of freedom. This is a well-known quantity with an expected value $E[\\chi_N^2]=N$ and a standard deviation of $\\sqrt{2N}$. Basically, what this means is that computing $\\phi_d$ should give us a value that is close to the number of data, $N$.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The Model Regularization, $\\phi_m$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are many options for choosing a model regularization, but the goal in determining a model regularization is the same: given that we have no unique solution, we must make assumptions in order to recast the problem in such a way that a solution exists. A general function used in 1D is as follows:\n",
      "\n",
      "$$\n",
      "\\phi_m = \\alpha_s \\int (m)^2 dx + \\alpha_x \\int \\left( \\frac{dm}{dx} \\right)^2 dx\n",
      "$$\n",
      "\n",
      "Each term in the above expression is an norm that measures characteristics about our model. The first term is a representation of the square of the Euclidean length for a continuous function, and therefore measures the length of the model, while the second term uses derivative information to measure the model's smoothness. Ususally the model regularization is defined with respect to a reference model. In the above, the refernce model would simply be zero, but choosing a non-zero reference model $m_{ref}$, yields the following:\n",
      "$$\n",
      "\\phi_m = \\alpha_s \\int (m-m_{ref})^2 dx + \\alpha_x \\int \\left( \\frac{d}{dx} (m-m_{ref}) \\right)^2 dx\n",
      "$$\n",
      "\n",
      "As before, we will discretize this expression. It is easiest to break up each term and treat them seperately, at first.\n",
      "We will denote each term of $\\phi_m$ as $\\phi_s$ and $\\phi_x$, respectively. Consider the first term. Translating the integral to a sum yields:\n",
      "\n",
      "$$\n",
      "\\phi_s = \\alpha_s \\int (m)^2 dx \\rightarrow \\sum_{i=1}^N \\int_{x_{i-1}}^{x_i} (m_i)^2 dx = \\sum_{i=1}^N m_i^2 (x_i - x_{i-1})\n",
      "$$\n",
      "\n",
      "Each spatial \"cell\" is $x_i - x_{i-1}$, which is the distance between nodes, as you may recall from the previous module. To simplify notation, we will use $\\Delta x_{n_i}$ to denote the *ith* distance between nodes:\n",
      "\n",
      "<img src=\"figures/1D_domain_dx.png\" width=\"40%\" height=\"40%\"> <br>\n",
      "\n",
      "\n",
      "We can then write $\\phi_s$ as:\n",
      "\n",
      "$$\n",
      "\\phi_s = \\alpha_s \\sum_{i=1}^N m_i^2 \\Delta x_{n_i} = \\alpha_s m^T W_s^T W_s m = \\alpha_s \\|W_s m\\|_2^2\n",
      "$$\n",
      "\n",
      "with:\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "W_d = \n",
      "\\begin{bmatrix}\n",
      "    \\frac{1}{\\sqrt{\\Delta x_{n_1}}} & 0 & 0 & \\cdots & 0\\\\\n",
      "    0 & \\frac{1}{\\sqrt{\\Delta x_{n_2}}} & 0 & \\cdots & 0\\\\\n",
      "    0 & 0 & \\frac{1}{\\sqrt{\\Delta x_{n_3}}} & \\cdots & \\vdots\\\\\n",
      "    0 & 0 & 0 & \\ddots & \\frac{1}{\\sqrt{\\Delta x_{n_N}}}\\\\    \n",
      "\\end{bmatrix}\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "For the second term, we will do a similar process. First, we will delineate $\\Delta x_{c_i}$ as the distance between cell centers:\n",
      "\n",
      "<img src=\"figures/1D_h_lengths_dx.png\" width=\"40%\" height=\"40%\"> <br>\n",
      "\n",
      "A discrete appriximation to the integral can be made by evaluating the derivative of the model based on how much it changes between the cell-centers, that is, we will take the average gradient between the $ith$ and $i+1th$ cells:\n",
      "\n",
      "$$\n",
      "\\phi_x = \\alpha_x \\int \\left( \\frac{dm}{dx} \\right)^2 dx \\rightarrow \\sum_{i=1}^{N-1} \\left( \\frac{m_{i+1}-m_i}{h_k}\\right) \\Delta x_{c_i} = m^T W_x^T W_x m = \\|W_x m\\|_2^2\n",
      "$$\n",
      "\n",
      "The matrix $W_x$ is a finite difference matrix constructed thus:\n",
      "\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "W_d = \n",
      "\\begin{bmatrix}\n",
      "    -\\frac{1}{\\sqrt{\\Delta x_{c_1}}} & \\frac{1}{\\sqrt{\\Delta x_{c_1}}} & 0 & \\cdots & 0\\\\\n",
      "    0 & -\\frac{1}{\\sqrt{\\Delta x_{c_2}}} & \\frac{1}{\\sqrt{\\Delta x_{c_2}}} & \\cdots & 0\\\\\n",
      "    0 & 0 & \\ddots & \\ddots & \\vdots\\\\\n",
      "    0 & 0 & 0 &  -\\frac{1}{\\sqrt{\\Delta x_{c_{N-1}}}} & \\frac{1}{\\sqrt{\\Delta x_{c_{N-1}}}}\\\\ \n",
      "    0 & 0 & 0 & 0 & 0\n",
      "\\end{bmatrix}\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "If $W_x$ is a matrix that is written as an $N \\times N$ matrix, then the last row is zero. The reason the last row is zero is because there are $N-1$ segments on which linear gradients have been defined. Effectively the two $1/2$ cells on each end are neglected. \n",
      "\n",
      "So to summarize, we have $\\phi_m = \\phi_s + \\phi_x$ with \n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "    \\phi_m & = \\phi_s + \\phi_x \\\\[0.4em]\n",
      "           & = \\alpha_s \\|W_s m\\|_2^2 + \\alpha_x \\|W_x m\\|_2^2 \\\\[0.4em]         \n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "Next, we will write this more compactly by stacking $W_s$ and $W_x$ into a matrix $W_m$ as follows\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "W_m = \n",
      "\\begin{bmatrix}\n",
      "    \\alpha_s W_s\\\\\n",
      "    \\alpha_x W_x\n",
      "\\end{bmatrix}\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "Now we can write $\\phi_m$ as the 2-norm of one matrix-vector operation:\n",
      "\n",
      "$$\n",
      "\\phi_m = \\| W_m m \\|_2^2\n",
      "$$\n",
      "\n",
      "As before, if we want to describe with with respect to a reference model $m_{ref}$ we could write:\n",
      "\n",
      "$$\n",
      "\\phi_m = \\| W_m (m-m_{ref})\\|_2^2\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Model Objective Function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we go back and recall what was discussed in the introduction, the model objective function casts the inverse problem as an optimization problem, and as mentioned, we will be using gradient-based optimization, so we will need to take derivatives. The complete model objective function that we are dealing with will contain both the data misfit and the model regularization. This means that we can write it as $\\phi$ as the sum of the two and then differentiate:\n",
      "\n",
      "$$\n",
      "\\phi = \\phi_d + \\beta \\phi_m\n",
      "$$\n",
      "For the linear problem we are considering\n",
      "$$\n",
      "\\phi_d = \\frac{1}{2}\\| W_d (Gm-d^{obs})\\|_2^2 = \\frac{1}{2}(Gm-d^{obs})^T W_d^T W_d (Gm-d^{obs})\n",
      "$$\n",
      "and\n",
      "$$\n",
      "\\phi_m = \\frac{1}{2} \\|W_m (m-m_{ref}) \\|^2_2 = \\frac{1}{2}(m-m_{ref})^T W_m^T W_m (m-m_{ref})\n",
      "$$\n",
      "\n",
      "To simplify the terms and see the math a little more clearly, let's note that $W_d(Gm-d^{obs})$, and $\\beta W_m(m-m_{ref})$ are simply vectors. And since we are taking the square of the 2-norm, all that we are really doing is taking the dot product of each vector with itself. So let $z=W_d(Gm-d^{obs})$, and let $y=W_m(m-m_{ref})$ where both $z$ and $y$ vectors are functions of $m$. So then:\n",
      "\n",
      "$$\n",
      "\\phi_d =  \\frac{1}{2}\\|z\\|_2^2 = \\frac{1}{2}z^T z \n",
      "$$<br>\n",
      "$$\n",
      "\\phi_m =  \\frac{1}{2}\\|y\\|_2^2 =\\frac{1}{2}y^T y \n",
      "$$\n",
      "\n",
      "\n",
      "To minimize this, we want to look at $\\nabla \\phi$. Using our compact expressions:\n",
      "$$\n",
      "\\phi = \\phi_d + \\beta \\phi_m = \\frac{1}{2}z^Tz + \\beta \\frac{1}{2}y^Ty \\\\ \n",
      "$$\n",
      "\n",
      "Taking the derivative with respect to $m$ yields:\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "\\frac{d \\phi}{dm}&  = \\frac{1}{2}  \\left(z^T \\frac{dz}{dm} +  z^T \\frac{dz}{dm} + \\beta y^T \\frac{dy}{dm} + \\beta y^T \\frac{dy}{dm}\\right)\\\\\\\\[0.6em]\n",
      "& = z^T \\frac{dz}{dm} + \\beta y^T \\frac{dy}{dm}\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "Note that \n",
      "$$\\frac{dz}{dm} = \\frac{d}{dm}(W_d(Gm-d^{obs})) = W_d G $$ \n",
      "\n",
      "and \n",
      "\n",
      "$$ \\frac{dy}{dm} = \\frac{d}{dm}(W_m (m-m_{ref})) = W_m $$\n",
      "\n",
      "Next, let's substitute both derivatives, our expressions for $z$ and $y$, apply the transposes, and rearange:<br>\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "\\frac{d \\phi}{dm} & = z^T \\frac{dz}{dm} + \\beta y^T \\frac{dy}{dm} \\\\[0.6em]\n",
      " & = (W_d(Gm-d^{obs}))^T W_d G + \\beta (W_m (m-m_{ref}))^T W_m\\\\[0.6em]\n",
      " & = (Gm-d^{obs})^T W_d^T W_d G + \\beta (m-m_{ref})^T W_m^T W_m \\\\[0.6em]\n",
      " & = ((Gm)^T - d^T) W_d^T W_d G + \\beta (m^T-m_{ref}^T)W_m^T W_m \\\\[0.6em]\n",
      " & = (m^T G^T - d^T) W_d^T W_d G + \\beta m^T W_m^T W_m - \\beta  m_{ref}^T W_m^T W_m \\\\[0.6em]\n",
      " & = m^T G^T W_d^T W_d G  - d^T W_d^T W_d G + \\beta m^T W_m^T W_m - \\beta  m_{ref}^T W_m^T W_m\\\\[0.6em]\n",
      " & = m^T G^T W_d^T W_d G  + \\beta m^T W_m^T W_m - d^T W_d^T W_d G - \\beta  m_{ref}^T W_m^T W_m\n",
      " \\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "Now we have an expression for the derivative of our equation that we can work with. Setting the gradient to zero and gathering like terms gives:<br>\n",
      "\n",
      "\\begin{equation} \n",
      "\\begin{split}\n",
      " m^T G^T W_d^T W_d G  + \\beta m^T W_m^T W_m = d^T W_d^T W_d G + \\beta  m_{ref}^T W_m^T W_m\\\\[0.6em]\n",
      " (G^T W_d^T W_d G  + \\beta W_m^T W_m)m = G^T W_d^T W_d d + \\beta  W_m^T W_m m_{ref}\\\\[0.6em]\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "From here we can do two things. First, we can solve for $m$, our recovered model:\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      " m = (G^T W_d^T W_d G  + \\beta W_m^T W_m)^{-1} (G^T W_d^T W_d d + \\beta  W_m^T W_m m_{ref})\\\\[0.6em]\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "\n",
      "Second, we can get the second derivative simply from the bracketed terms in the left hand side of the equation above:\n",
      "\\begin{equation} \n",
      "\\frac{d^2 \\phi}{dm^2} = G^T W_d^T W_d G  + \\beta W_m^T W_m\n",
      "\\end{equation}\n",
      "\n",
      "In the model problem that we are solving, second derivative information is not required to obtain a solution, however, in non-linear problems or situations when higher order information is required, it is useful to have this available when we need it. \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Choosing $\\alpha_s$,  $\\alpha_x$, and $\\beta$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have obtained a solution to our inverse prolem, there are non unique aspects that still remain, namely we do not have values for $\\alpha_s$, $\\alpha_x$, and $\\beta$. First consider the units for $\\alpha_s$ and $\\alpha_x$. Going back to the original expression\n",
      "\n",
      "$$\n",
      "\\phi_m = \\alpha_s \\int (m)^2 dx + \\alpha_x \\int \\left( \\frac{dm}{dx} \\right)^2 dx\n",
      "$$\n",
      "\n",
      "One can see that the units for $\\alpha_s$ are $[1/L]$ and those for $\\alpha_x$ are $[L]$. Taking the ratio of $\\alpha_x$ over $\\alpha_s$ gives:\n",
      "\n",
      "$$\n",
      "\\frac{\\alpha_x}{\\alpha_s} = L^2\n",
      "$$\n",
      "\n",
      "or, with respect to $L$ in one spatial domain $x$, we write:\n",
      "\n",
      "$$\n",
      "L_x = \\sqrt{\\frac{\\alpha_x}{\\alpha_s}}\n",
      "$$\n",
      "\n",
      "So a measure of the relative magnitudes of $\\alpha_x$ and $\\alpha_s$ can be determined by the length scale $L$, which ranges from the smallest spatial increment $\\Delta x$ (assuming equal spacing between cells) to the total length of the domain $N \\Delta x$\n",
      "\n",
      "$$\n",
      "\\Delta x < L < N \\Delta x\n",
      "$$\n",
      "\n",
      "As this ratio become much larger than 1, structure in the model is penalized. If this ratio is close to zero, then the smallest term dominates and models are rough.\n",
      "\n",
      "Choosing $\\beta$ is often done using one of three methods, (1) the \"Chifact\" method, (2) Generalized Cross Validation (GCV), or (3) the \"L-curve\" method. \n",
      "\n",
      "**Chifact**\n",
      "\n",
      "The Chifact method starts off with the assumption that noise in the data is random and Gaussian. If so, then the expected value for $\\phi_d \\approx N$. However, if we want to control how close the predicted data match measured data, we use:\n",
      "\n",
      "$$\n",
      "\\phi_d = \\text{Chifact} N\n",
      "$$\n",
      "\n",
      "where \"Chifact\" is a user defined parameter. If the data are noisy and/or non-Gaussian, then choosing Chifact greater than $1$ is appropriate. But if data are clean then Chifact can be less than $1$.\n",
      "\n",
      "\n",
      "**Generalized Cross Validation (GCV)**\n",
      "\n",
      "$$\n",
      "\\rightarrow \\text{this needs elaboration}\n",
      "$$\n",
      "\n",
      "\n",
      "**L-Curve**\n",
      "\n",
      "The L-curve method involves performing the inversion for several values of $\\beta$ and then plotting $\\phi_d$ and $\\phi_m$ on seperate axes. This generates what is known as the \"Tikhonov\" curve, shown below. \n",
      "\n",
      "<img src=\"figures/tikhonov3.gif\" width=\"20%\" height=\"20%\"> <br>\n",
      "\n",
      "From there a general approach is to choose the point on the Tikhonov curve of maximum curvature. \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}